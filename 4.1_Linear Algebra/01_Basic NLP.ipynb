{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Linear algebra is a powerful tool in Natural Language Processing (NLP) for handling and analyzing text data. Let's break down how it's used in various stages of NLP, starting from the basics and gradually building on the example. We'll go through tokenization, embedding, word/character statistics, and document-term matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Consider a simple sentence \"Hello World! This is an example.\"\n",
    "\n",
    "Tokenization is hte process of breaking down text into smaller units, such as words or phrases. It's the first step in preparing text for NLP tasks.\n",
    "\n",
    "At this stage linear algebra isn't directly applied. However, the outcome of tokenization sets the stage for using linear algebra in later setps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents: [['hello', 'world'], ['this', 'is', 'an', 'example']]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "documents = [\"Hello world!\", \"This is an example.\"]\n",
    "\n",
    "# Tokenization\n",
    "tokenized_docs = [doc.lower().replace(\"!\", \"\").replace(\".\", \"\").split() for doc in documents]\n",
    "print(\"Tokenized Documents:\", tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "After we tokenize the previous sentence, we end up with the tokens \"Hello\", \"World\", \"This\", \"is\", \"an\", \"example\".\n",
    "\n",
    "Embedding converts tokens into numerical vectors that represent the tokens in a high-dimensional space. Words with similar meanings are closer in this space.\n",
    "\n",
    "Each word is represented as a vector in say a 100-dimensional space. For example \"Hello\" could be represented as a vector `[0.5, -0.2,..., 0.1]`. \n",
    "\n",
    "If \"Hello\" and \"world\" are similar in some context, their vectors will be closer in the embedding space. Linear algebra operations like cosine similarity can measure this closeness.\n",
    "\n",
    "For simplicity, we'll simulate embeddings by assigning random vectors to each unique word. In practice, you'd use pre-trained models like `Word2Vec`, `GloVe`, or `fastText`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings: {'this': array([0.91214246, 0.36497166, 0.08030509, 0.19464836, 0.05500404]), 'hello': array([0.62905354, 0.80303779, 0.33973286, 0.22803549, 0.00468848]), 'world': array([0.77174385, 0.26235544, 0.35802619, 0.33824357, 0.9782507 ]), 'an': array([0.95591613, 0.57198752, 0.91885446, 0.38596546, 0.15889245]), 'is': array([0.18077022, 0.00288256, 0.17246388, 0.17740929, 0.63035225]), 'example': array([0.0714305 , 0.72558867, 0.63004908, 0.10963651, 0.85759371])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Unique words\n",
    "unique_words = set(word for doc in tokenized_docs for word in doc)\n",
    "\n",
    "# Simulated embeddings\n",
    "embeddings = {word: np.random.rand(5) for word in unique_words}  # 5-dimensional vectors\n",
    "print(\"Word Embeddings:\", embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word/Character Statistics\n",
    "\n",
    "Continuing with our tokens, let's analyse the word \"example\".\n",
    "\n",
    "This step involves calculating statistics like word frequency, character count, etc... It's useful for understanding the composition of texts.\n",
    "\n",
    "If we represent each character as a vector (e.g., a=1, b=2, ..., z=26), \"example\" can be represented as a matrix where each row corresponds to a character vector. Summing up these vectors can give us a statistical representation of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Lengths: {'this': 4, 'hello': 5, 'world': 5, 'an': 2, 'is': 2, 'example': 7}\n"
     ]
    }
   ],
   "source": [
    "word_lengths = {word: len(word) for word in unique_words}\n",
    "print(\"Word Lengths:\", word_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix (DTM)\n",
    "\n",
    "Consider two sentences: \"Hello World!\" and \"This is an example.\"\n",
    "\n",
    "A DTM is a matrix that describes the frequency of terms (words) that occur in a collection of documents. It's crucial for many tasks, including information retrieval and topic modeling.\n",
    "\n",
    "In terms of Linear Algebra:\n",
    "- each row represents a document.\n",
    "- each column represents a unique term for all documents.\n",
    "- entries in the matrix are the frequences of the terms in the documents. \n",
    "\n",
    "For our example, the DTM might look like this if we only consider unique words:\n",
    "\n",
    "\n",
    "||Hello|world|This|is|an|example|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Doc1|1|1|0|0|0|0|\n",
    "|Doc2|0|0|1|1|1|1|\n",
    "\n",
    "With this matrix, we can perform operations like calculating the cosine similarity between documents, which tells us how similar two documents are based on their word usage. This involves linear algebra operations like dot products and norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix:\n",
      "\n",
      "{'this', 'hello', 'world', 'an', 'is', 'example'}\n",
      "[[0. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize DTM\n",
    "dtm = np.zeros((len(tokenized_docs), len(unique_words)))\n",
    "\n",
    "# Mapping of words to columns\n",
    "word_to_index = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "# Fill DTM\n",
    "for doc_idx, doc in enumerate(tokenized_docs):\n",
    "    for word in doc:\n",
    "        word_idx = word_to_index[word]\n",
    "        dtm[doc_idx, word_idx] += 1\n",
    "\n",
    "print(\"Document-Term Matrix:\\n\")\n",
    "print(unique_words)\n",
    "print(dtm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity\n",
    "\n",
    "To demonstrate document similarity, we'll use the Document-Term Matrix (DTM) created in the previous steps. Document similarity can be calculated in various ways, but one common method is using the cosine similarity measure. \n",
    "\n",
    "Cosine similarity calculates the cosine of the angle between two non-zero vectors of an inner product space, which in this context are the vectors representing our documents in the DTM. This measure helps us understand how similar two documents are in terms of their word usage.\n",
    "\n",
    "First, let's calculate the cosine similarity between two documents. We'll use the `numpy` library for this, as it provides efficient linear algebra operations.\n",
    "\n",
    "### Calculating Cosine Similarity\n",
    "\n",
    "The cosine similarity between two vectors $A$ and $B$ is given by the formula:\n",
    "\n",
    "$cosine\\: similarity = \\frac{A.B}{\\left\\| A\\right\\| \\left\\| B\\right\\| } $\n",
    "\n",
    "where:\n",
    "- $A.B$ is the dot product of vectors $A$ and $B$,\n",
    "- ${\\left\\| A\\right\\|}$ and  ${\\left\\| B\\right\\| }$ are the Euclidean norms (or magnitudes) of vectors $A$ and $B$ respectively. This denotes how far away the point is from the origin.\n",
    "\n",
    "For example, consider `A=[1, 2, 3]` and `B=[4, 5, 6]`:\n",
    "- $A.B$ = $(1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32$\n",
    "- ${\\left\\| A\\right\\|}$ = $3.74$\n",
    "- ${\\left\\| B\\right\\|}$ = $8.77$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$cosine\\: similarity = \\frac{32}{3.74*8.77} = 0.97 $\n",
    "\n",
    "This means these two matrices are very close to each other.\n",
    "\n",
    "Let's apply it to our documents now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between Document 1 and Document 2: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "doc1 = dtm[0]  # Vector representing the first document\n",
    "doc2 = dtm[1]  # Vector representing the second document\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = cosine_similarity(doc1, doc2)\n",
    "print(f\"Cosine similarity between Document 1 and Document 2: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the similarity between `Document 1` and `Document 2` is nothing.\n",
    "\n",
    "If you had multiple documents, you can create a similarity matrix to compare all documents with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "num_docs = dtm.shape[0]\n",
    "similarity_matrix = np.zeros((num_docs, num_docs))\n",
    "\n",
    "for i in range(num_docs):\n",
    "    for j in range(num_docs):\n",
    "        if i != j:\n",
    "            similarity_matrix[i, j] = cosine_similarity(dtm[i], dtm[j])\n",
    "        else:\n",
    "            similarity_matrix[i, j] = 1  # Document is perfectly similar to itself\n",
    "\n",
    "print(\"Similarity Matrix:\\n\", similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix provides a comprehensive view of how each document relates to every other document in your corpus, based on their content. High values (close to 1) indicate strong similarity, while low values (close to 0) indicate low similarity. This approach is very useful for clustering documents, recommending content, or detecting duplicate documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Linear algebra in NLP allows us to convert textual information into numerical form, enabling the application of mathematical models and algorithms to analyze and understand language. Each step, from tokenization to embedding and constructing DTMs, builds upon the previous to enrich the analysis capabilities in NLP tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
